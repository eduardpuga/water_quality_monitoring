{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WATER WUALITY MONITORING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "- **Sensor ID:** Unique identifier for the sensor collecting the data.\n",
    "- **Location:** The geographical location where the data was collected.\n",
    "- **Timestamp:** The date and time when the data was recorded.\n",
    "- **Temperature:** The water temperature measured by the sensor.\n",
    "- **pH:** The pH level of the water.\n",
    "- **Conductivity:** The conductivity of the water.\n",
    "- **Additional Attributes (optional):** Attributes like turbidity, dissolved oxygen, etc., depending on the available sensors and their capabilities.\n",
    "\n",
    "### 1. POST\n",
    "- **Purpose:** Used to create a new water quality record on the server.\n",
    "- **URL Format:** Typically targets the base URL of the resource collection.\n",
    "- **Example:** Creating a new water quality record.\n",
    "- **Endpoint:** `/api/water-quality`\n",
    "- **Method:** POST\n",
    "- **Request Body:**\n",
    "    ```json\n",
    "    {\n",
    "      \"sensor_id\": \"sensor123\",\n",
    "      \"temperature\": 22.5,\n",
    "      \"pH\": 7.2,\n",
    "      \"conductivity\": 500,\n",
    "      \"location\": \"Lake A\",\n",
    "      \"timestamp\": \"2024-07-31T10:00:00Z\"\n",
    "    }\n",
    "    ```\n",
    "- **Responses:**\n",
    "  - `201:` Record successfully created.\n",
    "  - `400:` Invalid data format.\n",
    "\n",
    "### 2. GET\n",
    "- **Purpose:** Used to retrieve water quality records.\n",
    "- **URL Format:** Typically targets a specific resource or the base URL of the resource collection.\n",
    "- **Example:** Retrieving water quality records for a specific location.\n",
    "- **Endpoint:** `/api/water-quality`\n",
    "- **Method:** GET\n",
    "- **Request Parameters:**\n",
    "  - `start_date` (string): The start date for the records to retrieve.\n",
    "  - `end_date` (string): The end date for the records to retrieve.\n",
    "  - `location` (string): The location for which to retrieve water quality records.\n",
    "- **Response Body:**\n",
    "    ```json\n",
    "    [\n",
    "      {\n",
    "        \"id\": 1,\n",
    "        \"sensor_id\": \"sensor123\",\n",
    "        \"temperature\": 22.5,\n",
    "        \"pH\": 7.2,\n",
    "        \"conductivity\": 500,\n",
    "        \"location\": \"Lake A\",\n",
    "        \"timestamp\": \"2024-07-31T10:00:00Z\"\n",
    "      }\n",
    "    ]\n",
    "    ```\n",
    "- **Responses:**\n",
    "  - `200:` OK.\n",
    "  - `404:` No records found.\n",
    "\n",
    "### 3. PUT\n",
    "- **Purpose:** Used to update an existing water quality record on the server.\n",
    "- **URL Format:** Typically targets a specific resource.\n",
    "- **Example:** Updating the temperature of a water quality record.\n",
    "- **Endpoint:** `/api/water-quality/{record_id}`\n",
    "- **Method:** PUT\n",
    "- **Request Parameters:**\n",
    "  - `record_id` (integer): The ID of the water quality record to update.\n",
    "- **Request Body:**\n",
    "    ```json\n",
    "    {\n",
    "      \"temperature\": 23.5\n",
    "    }\n",
    "    ```\n",
    "- **Responses:**\n",
    "  - `200:` Record successfully updated.\n",
    "  - `404:` Record not found.\n",
    "  - `400:` Invalid data format.\n",
    "\n",
    "### 4. DELETE\n",
    "- **Purpose:** Used to delete an existing water quality record from the server.\n",
    "- **URL Format:** Typically targets a specific resource.\n",
    "- **Example:** Deleting a water quality record.\n",
    "- **Endpoint:** `/api/water-quality/{record_id}`\n",
    "- **Method:** DELETE\n",
    "- **Request Parameters:**\n",
    "  - `record_id` (integer): The ID of the water quality record to delete.\n",
    "- **Responses:**\n",
    "  - `200:` Record successfully deleted.\n",
    "  - `404:` Record not found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Machine Learning\n",
    "\n",
    "### Indicators Quality of the Data and Measures to Mitigate Effects of Poor Data Quality\n",
    "\n",
    "- **Missing Values:**\n",
    "  - **Detection:** We should look for any missing values in the dataset.\n",
    "  - **Mitigation:** If they are really few, we can remove the records or maybe we can impute them.\n",
    "\n",
    "- **Outliers:**\n",
    "  - **Detection:** Identify data points that are significantly different from others.\n",
    "  - **Mitigation:** We can use IQR or Z-scores (boxplots and scatter plots are useful). Depending on the data and field, we can remove them or we may have to be aware of them.\n",
    "\n",
    "- **Data Distribution:**\n",
    "  - **Analysis:** Distribution of data in each column, histograms, summary statistics (mean, median, standard deviation), unbalanced categorical classes.\n",
    "  - **Mitigation:** For example, if we have a lot of categories and some have little proportions like 0.5% or 1%, we could join them and put others in order to have our models achieve better accuracy later. We can also use oversampling with data generation, apply log transformation to normalize, or adjust class weights.\n",
    "\n",
    "- **Consistency and Validity:**\n",
    "  - **Consistency:** Ensure that the data is in the same format. For instance, in age, some might categorize as 'avi' and some as 'grandpa'.\n",
    "  - **Validity:** Apply constraints (e.g., enforce a limit on age values).\n",
    "\n",
    "- **Handle Duplicates:**\n",
    "  - **Detection:** Check for duplicates in the records.\n",
    "  - **Mitigation:** Remove them if found.\n",
    "\n",
    "- **Correlation:**\n",
    "  - **Analysis:** Check the correlation of the values.\n",
    "  - **Mitigation:** If they are correlated, they bring the same information (redundant information).\n",
    "\n",
    "### Machine Learning Model\n",
    "\n",
    "- **Regression Model:**\n",
    "  - **Use Case:** If we want to do a simpler model, we could use a regression model.\n",
    "  - **Pros:** Simple and quick.\n",
    "  - **Cons:** Assumes linear relationship between features and target, which might not hold in complex problems.\n",
    "\n",
    "- **Deep Learning:**\n",
    "  - **Use Case:** Can capture complex non-linear relationships in the data.\n",
    "  - **Pros:** Powerful and versatile.\n",
    "  - **Cons:** Prone to overfitting, especially with small datasets. Techniques to mitigate overfitting include early stopping, dropouts, and regularization.\n",
    "\n",
    "- **Transformers:**\n",
    "  - **Use Case:** Suitable for capturing long-term dependencies and relationships in the data.\n",
    "  - **Pros:** \n",
    "    - Can handle large amounts of data and capture intricate patterns.\n",
    "    - Excellent at understanding contextual relationships due to self-attention mechanisms.\n",
    "  - **Cons:**\n",
    "    - Requires substantial computational resources.\n",
    "    - Needs a large amount of data for effective training.\n",
    "    - Data needs to be formatted into sequences, which can add complexity to preprocessing.\n",
    "\n",
    "- **Time Series Forecasting Models:**\n",
    "  - **Use Case:** Specifically designed to capture temporal dependencies and trends in time-series data.\n",
    "  - **Pros:**\n",
    "    - Tailored for time-dependent data, making them highly effective for forecasting.\n",
    "    - Can model trends, seasonality, and cyclic behaviors in data.\n",
    "  - **Cons:**\n",
    "    - May require extensive historical data to capture long-term patterns.\n",
    "    - Complex models can be prone to overfitting, especially with limited data.\n",
    "    - Need careful handling of non-stationary data and potential external influencing factors.\n",
    "\n",
    "\n",
    "### Other Solutions Based on This Data\n",
    "\n",
    "1. **Water Quality Forecasting Dashboard:**\n",
    "   - **Description:** We could create a comprehensive dashboard that not only predicts future water quality but also visualizes historical data, trends, and potential future scenarios.\n",
    "\n",
    "2. **Real-Time Water Quality Mapping:**\n",
    "   - **Description:** We could develop a real-time water quality mapping system to visualize water quality across different regions.\n",
    "\n",
    "3. **Real-Time Public Health Alerts:**\n",
    "   - **Description:** Develop a system that issues real-time alerts to the public about water quality issues that could impact health, such as contamination, unsafe swimming conditions, or potential increases in waterborne illnesses. This is crucial as many people drink water from the sink.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Life Cycle and Operations\n",
    "\n",
    "### Tools\n",
    "I especially like Jupyter Notebooks and Visual Studio Code. I think the debugging works pretty well and allows you to work efficiently. I also used IntelliJ for backend frameworks in the past, but not for writing Python.\n",
    "\n",
    "For libraries, there are lots of them:\n",
    "\n",
    "- **Data Manipulation:** pandas, numpy\n",
    "- **Data Visualization:** matplotlib, seaborn\n",
    "- **Machine Learning:** scikit-learn, XGBoost, LightGBM, TensorFlow, Keras, PyTorch\n",
    "- **Monitoring and Logging:** MLflow, TensorBoard, Prometheus, Grafana\n",
    "\n",
    "### Deployment\n",
    "\n",
    "Explain first the part of GitHub and Google Cloud Platform (CI/CD pipelines), that you can create with Docker image containing the model and its dependencies and YAML files for deployment configurations, and when a push is done, it can be trained in the cloud or deployed.\n",
    "\n",
    "I think both have their advantages and disadvantages:\n",
    "\n",
    "- **Cron Jobs:** \n",
    "  - **Purpose:** If we have a routine for new data refresh or new models from time to time, we can use cron jobs to automate the process. \n",
    "  - **Process:** Get the data again, preprocess it, and then train the model again. This helps in getting better models in the future.\n",
    "\n",
    "- **HTTP Requests:** \n",
    "  - **Purpose:** If we don't have a system for routine updates, we could use HTTP requests, similar to the git push explained before.\n",
    "  - **Implementation:** Although I have never done it with HTTP, we should:\n",
    "\n",
    "#### 1. POST\n",
    "- **Purpose:** Used to send data to the server and receive predictions based on the provided data.\n",
    "- **URL Format:** Targets the endpoint responsible for making predictions.\n",
    "- **Example:** Sending data to get water quality predictions.\n",
    "- **Endpoint:** `/api/predict`\n",
    "- **Method:** POST\n",
    "- **Request Body:**\n",
    "    - **Description:** The request body should contain the data for which predictions are needed.\n",
    "    - **Example:**\n",
    "    \n",
    "      ```json\n",
    "      {\n",
    "        \"feature1\": 22.5,\n",
    "        \"feature2\": 7.2,\n",
    "        \"feature3\": 5\n",
    "      }\n",
    "      ```\n",
    "- **Response Body:**\n",
    "    - **Description:** The response will contain the predictions for the provided data.\n",
    "    - **Example:**\n",
    "      ```json\n",
    "      [0.5]\n",
    "      ```\n",
    "\n",
    "### Monitor\n",
    "\n",
    "I haven't touched much on monitoring models in the cloud, but most platforms provide metrics about performance and predictions. I would use the metrics of train, validation, and test to check performance. Also, the F1 score (precision and recall) is useful for imbalanced datasets as it checks true positives, false positives, and false negatives.\n",
    "\n",
    "Also, there are tools like Prometheus and Grafana, or even MLflow, to manage and deploy models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Database\n",
    "### MongoDB:\n",
    "- **Pros:**\n",
    "  - Schema flexibility\n",
    "  - Horizontal scalability\n",
    "  - High write throughput\n",
    "  - Document-oriented storage\n",
    "  - Powerful aggregation framework\n",
    "- **Cons:**\n",
    "  - Eventual consistency\n",
    "  - Limited ACID transactions\n",
    "  - Less efficient for complex relational queries\n",
    "- **Uses:**\n",
    "  - CMS\n",
    "  - IoT\n",
    "  - Real-time analytics\n",
    "  - E-commerce\n",
    "  - Big data applications\n",
    "\n",
    "### PostgreSQL:\n",
    "- **Pros:**\n",
    "  - Strong ACID compliance\n",
    "  - Advanced querying capabilities\n",
    "  - Data integrity\n",
    "  - Full-text search\n",
    "  - Extensibility\n",
    "- **Cons:**\n",
    "  - Schema rigidity\n",
    "  - Primarily vertical scalability\n",
    "  - Complex horizontal scaling\n",
    "- **Uses:**\n",
    "  - Transactional systems\n",
    "  - Data warehousing\n",
    "  - Enterprise applications\n",
    "  - Geospatial data\n",
    "  - Business intelligence\n",
    "\n",
    "### Choose MongoDB:\n",
    "\n",
    "#### High Data Ingestion Rates:\n",
    "- **Requirement:** We need to continuously ingest large volumes of data to keep our visualizations up to date.\n",
    "- **Advantage:** MongoDB is optimized for high write throughput, making it ideal for real-time data ingestion from various sensors.\n",
    "\n",
    "#### Frequent Data Reads:\n",
    "- **Requirement:** Our project involves reading large datasets to feed our machine learning models.\n",
    "- **Advantage:** MongoDB's efficient read performance, especially when properly indexed, ensures that we can quickly access the data needed for model training and analytics.\n",
    "\n",
    "#### Schema Flexibility:\n",
    "- **Requirement:** The nature of our data from various sensors means that the structure can evolve over time.\n",
    "- **Advantage:** MongoDB's schema-less design allows us to handle this variability seamlessly, without the need for complex schema migrations.\n",
    "\n",
    "#### Scalability:\n",
    "- **Requirement:** As our project grows, we will need to scale our database.\n",
    "- **Advantage:** MongoDB's horizontal scalability allows us to distribute the data across multiple nodes easily, ensuring that we can handle increasing data volumes without performance degradation.\n",
    "\n",
    "#### Reduced Need for Joins:\n",
    "- **Requirement:** Given that we are not a bank or large enterprise, our use case does not require complex transactions or joins.\n",
    "- **Advantage:** MongoDB's document-oriented approach fits well with our data structure and access patterns, reducing the overhead of managing complex relationships.\n",
    "\n",
    "#### Eventual Consistency:\n",
    "- **Requirement:** While strong consistency is crucial for financial institutions, our project can tolerate eventual consistency.\n",
    "- **Advantage:** This trade-off allows us to benefit from MongoDB's performance and flexibility.\n",
    "\n",
    "### Conclusion:\n",
    "MongoDB is well-suited for our machine learning and analytics project due to its high write throughput, efficient read performance, schema flexibility, and scalability. These features align well with our need for real-time data ingestion, frequent data reads for model training, and the ability to handle evolving data structures.\n",
    "\n",
    "\n",
    "### Collections\n",
    "\n",
    "## Embedded vs. Referenced in MongoDB\n",
    "\n",
    "### Embedded is Nice When:\n",
    "- **One-to-One Relationship:** For instance, a sensor can only be in one location. (Consider if each location (e.g., Lake A) has one sensor or multiple sensors).\n",
    "- **Combined Information Needs:** When we need information about the sensor along with its location (eliminating the need for joins).\n",
    "- **Static Location:** If the sensor's location doesn't change often.\n",
    "- **Minimal Information:** If there's not too much information to store.\n",
    "\n",
    "### Referenced is Nice When:\n",
    "- **Dynamic Location:** The sensor's location changes often.\n",
    "- **Shared Location:** If many sensors share the same location (reducing redundancy).\n",
    "- **Multiple Sensors per Location:** If one location has many sensors.\n",
    "\n",
    "### Decision:\n",
    "We assume that one location has only one sensor, the sensors are static and do not change places, so we will use embedded documents.\n",
    "\n",
    "### Example Collections\n",
    "\n",
    "#### Sensors Collection (with Embedded Location):\n",
    "```json\n",
    "{\n",
    "  \"_id\": \"sensor123\",\n",
    "  \"sensor_name\": \"Temperature Sensor\",\n",
    "  \"sensor_type\": \"Temperature\",\n",
    "  \"location\": {\n",
    "    \"name\": \"Lake A\",\n",
    "    \"latitude\": 40.7128,\n",
    "    \"longitude\": -74.0060,\n",
    "    \"description\": \"A large freshwater lake\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Water quality records collection (referencing snesor):\n",
    "```json\n",
    "{\n",
    "  \"_id\": \"record123\",\n",
    "  \"sensor_id\": \"sensor123\",\n",
    "  \"timestamp\": \"2024-07-31T10:00:00Z\",\n",
    "  \"temperature\": 22.5,\n",
    "  \"pH\": 7.2,\n",
    "  \"conductivity\": 500,\n",
    "  \"additional_attributes\": {\n",
    "    \"turbidity\": 5.0,\n",
    "    \"dissolved_oxygen\": 8.0\n",
    "  }\n",
    "}\n",
    "```\n",
    "### Configuration Database\n",
    "\n",
    "The decision to use embedding or referencing is important during the design phase. We should also implement:\n",
    "\n",
    "- **Replication:** Ensures high availability and protection against data loss.\n",
    "- **Sharding:** Distributes data across multiple machines for very large datasets. Each shard contains a subset of the data, providing horizontal scalability and load balancing, which increases performance.\n",
    "- **Indexes:** Creating indexes on frequently queried fields can speed up read operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Deployment\n",
    "\n",
    "For deployment, I would use Kubernetes because it is the most effective way to deploy both the database and the models.\n",
    "(I used heroku because its free in his moment)\n",
    "I think there are two main approaches:\n",
    "\n",
    "#### Use Server of Your Own (On-Premises):\n",
    "\n",
    "- **Pros:**\n",
    "  - Full control over the infrastructure.\n",
    "  - Customization according to specific needs.\n",
    "  - Enhanced data privacy.\n",
    "\n",
    "- **Cons:**\n",
    "  - Really expensive due to infrastructure and maintenance costs.\n",
    "  - Requires a dedicated IT team for maintenance.\n",
    "  - Sometimes difficult scalability.\n",
    "\n",
    "#### Use Cloud Providers (AWS, GCP, Azure):\n",
    "AWS is the most used and most cloud provider (rnge of servics, entreprise grade applcaitions)\n",
    "GCP is great for data analytics, ML, (so maybe this case)\n",
    "Azure is grreat for companies they alredy have a range of microsoft prodcuts\n",
    "- **Pros:**\n",
    "  - Scalability is easily managed.\n",
    "  - Kubernetes is easy to use with cloud providers.\n",
    "  - High availability of services.\n",
    "\n",
    "- **Cons:**\n",
    "  - Less expensive than owning your own server, but still incurs pay-as-you-go pricing.\n",
    "  - Dependency on another company.\n",
    "  - More challenges in terms of data privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing, Documentation, and Version Control\n",
    "\n",
    "### Testing\n",
    "\n",
    "Here, I would distinguish different kinds of tests:\n",
    "\n",
    "- **Unit Testing:**\n",
    "  - Useful for testing individual components or functions to ensure they work as expected.\n",
    "  - Tools: `unittest`, `pytest`, `assert`.\n",
    "\n",
    "- **Integration Testing:**\n",
    "  - Tests interactions between different components, such as between the API and the database.\n",
    "\n",
    "- **End-to-End Testing:**\n",
    "  - Verifies the entire application flow from start to finish, such as retrieving data, training the model, and deploying it.\n",
    "\n",
    "### Documentation\n",
    "\n",
    "For documentation, I think using docstrings and commenting on the code functions is really useful. For example:\n",
    "\n",
    "```python\n",
    "def add_record(data):\n",
    "    \"\"\"\n",
    "    Add a new water quality record to the database.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary containing water quality information.\n",
    "            - sensor_id (str): The ID of the sensor.\n",
    "            - temperature (float): The temperature reading.\n",
    "            - pH (float): The pH level.\n",
    "            - conductivity (int): The conductivity value.\n",
    "            - location (str): The location name.\n",
    "            - timestamp (str): The time of the reading.\n",
    "\n",
    "    Returns:\n",
    "        dict: Response message and status.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "There are also technologies that help you create documentation, such as:\n",
    "\n",
    "- **Sphinx:** Generates documentation for Python projects in HTML, PDF formats.\n",
    "- **Swagger:** Used for API documentation.\n",
    "\n",
    "### Version Control\n",
    "\n",
    "We will create a Git repository and link it to GitHub.\n",
    "\n",
    "**In local:**\n",
    "\n",
    "```bash\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"Initial commit: Set up project structure\"\n",
    "```\n",
    "\n",
    "**In the github:**\n",
    "\n",
    "- Create a repository named water_quality_monitoring.\n",
    "- Link the local repository to GitHub:\n",
    "```bash\n",
    "git remote add origin https://github.com/eduardpuga/water_quality_monitoring.git\n",
    "git push -u origin master\n",
    "```\n",
    "\n",
    "Create a development branch:\n",
    "\n",
    "```bash\n",
    "git checkout -b develop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
